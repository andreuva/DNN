{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTEBOOK FOR TRAINING THE MODEL WITH RESNET ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE MODULES NEEDED\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.python.keras.utils import losses_utils\n",
    "#import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.preprocessing import normalize\n",
    "from classification_models.tfkeras import Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the parameters of the network and the save/log/data directories\n",
    "# Change parameters here...\n",
    "sizex = 500\n",
    "sizey = 500\n",
    "sizez = 3\n",
    "rand_seed = 666\n",
    "num_datasets = -1\n",
    "batch_size = 32 * 4\n",
    "nb_epochs = 400\n",
    "validation_ratio = 0.2\n",
    "ds_size = 405\n",
    "buf_size = ds_size*2\n",
    "\n",
    "# Parameter to also use the images with the dust rendered\n",
    "use_dust = False\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Define the desired resnet model\n",
    "resnet_select = 'resnet18'\n",
    "constrained_resnet = False\n",
    "\n",
    "model_name = resnet_select + '_bs' + str(batch_size)\n",
    "if use_dust: model_name += '_dust'\n",
    "\n",
    "snapshot_weights = 'models/best_'+model_name+'.hdf5'\n",
    "last_snapshot_weights = 'models/last_'+model_name+'.hdf5'\n",
    "json_name = 'models/json'+model_name+'.json'\n",
    "trained_model = 'models/trained'+model_name+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function from the cosine similarity impleted in tensorflow\n",
    "class myCosineSimilarity(tf.keras.losses.Loss):\n",
    "    def __init__(self,\n",
    "               axis=-1,\n",
    "               reduction=losses_utils.ReductionV2.AUTO,\n",
    "               name='myCosineSimilarity'):\n",
    "        super(myCosineSimilarity, self).__init__(reduction=reduction, name=name)\n",
    "        self._axis = axis\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        cossim = lambda y, p_y: -1*tf.keras.backend.abs(tf.keras.losses.cosine_similarity(y, p_y, axis=self._axis))\n",
    "        return cossim(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Defining the model in all the devices (GPU's) available\n",
    "# import the correct resnet model\n",
    "ResNet, preprocess_input = Classifiers.get(resnet_select)\n",
    "with strategy.scope():\n",
    "    # Build the model\n",
    "    model = ResNet(input_shape=(sizex, sizey, sizez), include_top=False)\n",
    "\n",
    "    if constrained_resnet:\n",
    "        for i in range(88, 29, -1):\n",
    "            model._layers.pop()\n",
    "\n",
    "    globavg = tf.keras.layers.GlobalAveragePooling2D()(model.layers[-1].output)\n",
    "    output = tf.keras.layers.Dense(2, activation=None)(globavg)\n",
    "    model = tf.keras.models.Model(inputs=model.inputs, outputs=output)\n",
    "    \n",
    "    # Use existing best weights if available...\n",
    "    if os.path.isfile(last_snapshot_weights):\n",
    "        print('loaded weights')\n",
    "        model.load_weights(last_snapshot_weights)\n",
    "    \n",
    "    # Compile the model specifying the optimazer (sgd) and the custom loss function and other metrics\n",
    "    model.compile(optimizer= tf.keras.optimizers.SGD(),\n",
    "                  loss= myCosineSimilarity(), metrics=[tf.keras.losses.cosine_similarity,\n",
    "                                                       tf.keras.metrics.mean_absolute_error,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See a sumary of the model with all the layers and parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the .npz files\n",
    "numpy_datasets = glob.glob(data_dir+'dataset*')\n",
    "\n",
    "# load the data into a dictionary from npz files\n",
    "data_dict = {}\n",
    "for dataset in numpy_datasets[:num_datasets]:\n",
    "    with np.load(dataset) as data:\n",
    "        if 'y' in data_dict.keys():\n",
    "            data_dict['x_dust']   = np.append( data_dict['x_dust'], data['x_dust'],axis=0)\n",
    "            data_dict['x_nodust'] = np.append( data_dict['x_nodust'], data['x_nodust'],axis=0)\n",
    "            data_dict['y']        = np.append( data_dict['y'], data['y'],axis=0)\n",
    "            data_dict['ids']      = np.append( data_dict['ids'], data['id'],axis=0)\n",
    "        else:\n",
    "            data_dict['x_dust']   = data['x_dust']\n",
    "            data_dict['x_nodust'] = data['x_nodust']\n",
    "            data_dict['y']        = data['y']\n",
    "            data_dict['ids']      = data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dictionary (100 images of 500x500x3 = 4 files of 25 images) before filtering\n",
    "print(data_dict['x_nodust'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the galaxies that have less than 1e10 solar masses\n",
    "bad_ids = [[galaxy_id,i] for galaxy_id,i in zip(data_dict['ids'],range(len(data_dict['ids']))) if 'e09' in galaxy_id or 'e10' in galaxy_id ]\n",
    "bad_ids = np.array(bad_ids)\n",
    "remove_galaxies = np.array(bad_ids[:,1],dtype=int)\n",
    "print('there are ', len(bad_ids), ' galaxies to remove')\n",
    "#print(remove_galaxies)\n",
    "filtered_dict = {}\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = np.delete(data_dict[key],remove_galaxies, axis=0)\n",
    "    \n",
    "print('Still have ', len(data_dict['ids']), ' galaxies left')\n",
    "# check that we have filtered the correct galaxies\n",
    "print(data_dict['ids'][:5])\n",
    "# check the shape of the dictionary after filtering\n",
    "print(\"shape of image's array\",data_dict['x_dust'].shape)\n",
    "# check the shape of our dictionary in te labels entry\n",
    "print(\"shape of the label's array\",data_dict['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the normaliced vectors in 3D\n",
    "data_dict['y_norm'] = normalize(data_dict['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to do the augmentation\n",
    "def random_flip_lr(image, label, seed=None):\n",
    "    img = tf.image.random_flip_left_right(image, seed=seed)\n",
    "    if tf.reduce_all(tf.equal(img,image)):\n",
    "        label_f = label\n",
    "    else:\n",
    "        label_f = tf.convert_to_tensor([-label[0],label[1]])\n",
    "    return (img, label_f)\n",
    "\n",
    "def random_flip_ud(image, label, seed=None):\n",
    "    img = tf.image.random_flip_up_down(image, seed=seed)\n",
    "    if tf.reduce_all(tf.equal(img,image)):\n",
    "        label_f = label\n",
    "    else:\n",
    "        label_f = tf.convert_to_tensor([label[0],-label[1]])\n",
    "    return (img,label_f)\n",
    "\n",
    "\n",
    "def random_rot(image, label, seed=None):\n",
    "    number_of_intervalls = 24 # 15 deg steps\n",
    "    \n",
    "    rad = tf.random.uniform(shape=[1], minval=0, maxval=2*math.pi, dtype=tf.float32)\n",
    "    rad = rad//(2*math.pi/number_of_intervalls) * (2*math.pi/number_of_intervalls)\n",
    "    #rad = tf.math.multiply(deg, tf.constant(math.pi/180, dtype=tf.float32))\n",
    "    img = tfa.image.rotate(image, rad, interpolation = 'BILINEAR')\n",
    "    \n",
    "    x = tf.math.multiply(tf.math.cos(-rad), label[0]) - tf.math.multiply(tf.math.sin(-rad), label[1])\n",
    "    y = tf.math.multiply(tf.math.sin(-rad), label[0]) + tf.math.multiply(tf.math.cos(-rad), label[1])\n",
    "    label_f = tf.reshape(tf.convert_to_tensor([x, y]), [2])\n",
    "    return (img,label_f)\n",
    "\n",
    "def augment(img, label):\n",
    "    img_f,label_f = random_flip_lr(img,label)\n",
    "    img_f,label_f = random_flip_ud(img_f,label_f)\n",
    "    img_f,label_f = random_rot(img_f,label_f)\n",
    "    return (img_f, label_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the autotune option from TF\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Defining the train and validation dataset from the existing dictionary, shufling and batching\n",
    "# First we do the ones without dust, then if duts is activated we make it also with dust and mix both\n",
    "input_ds_nodust = (\n",
    "    tf.data.Dataset.from_tensor_slices((data_dict['x_nodust'].astype(np.float32),\n",
    "                                        data_dict['y_norm'][:,:2].astype(np.float32)))\n",
    "                    # Use seed to ensure we always have the same validation data set!\n",
    "                   .shuffle(ds_size, seed = rand_seed, reshuffle_each_iteration = False))\n",
    "\n",
    "# take a part of the dataset (we take all but there's the option to take just a part of it with the ds_size parameter)\n",
    "input_ds_nodust = input_ds_nodust.take(ds_size)\n",
    "# compute the val_size dataset in terms of the ds_size and the validation_ratio that we have\n",
    "val_size = math.floor(ds_size * validation_ratio)\n",
    "\n",
    "# cache and augment the dataset (THE ORDER IS IMPORTANT!!)\n",
    "validation_ds_nodust_aug =  (input_ds_nodust.take(val_size).cache()\n",
    "                            .map(augment, num_parallel_calls=AUTO))\n",
    "train_ds_nodust_aug      =  (input_ds_nodust.skip(val_size).cache()\n",
    "                            .map(augment, num_parallel_calls=AUTO))\n",
    "\n",
    "# if dust is activated compute the dust datasets\n",
    "if use_dust:\n",
    "    input_ds_dust = (\n",
    "        tf.data.Dataset.from_tensor_slices((data_dict['x_dust'].astype(np.float32),\n",
    "                                            data_dict['y_norm'][:,:2].astype(np.float32)))\n",
    "                       .shuffle(ds_size, seed = rand_seed, reshuffle_each_iteration = False))\n",
    "    \n",
    "    input_ds_dust = input_ds_dust.take(ds_size)\n",
    "    validation_ds_dust_aug = (input_ds_dust.take(val_size).cache()\n",
    "                              .map(augment, num_parallel_calls=AUTO))\n",
    "    train_ds_dust_aug =  (input_ds_dust.skip(val_size).cache()\n",
    "                          .map(augment, num_parallel_calls=AUTO))\n",
    "    \n",
    "    # concatenate both datasets (dust and no dust) and repeat the data and batch it\n",
    "    train_ds = (train_ds_dust_aug.concatenate(train_ds_nodust_aug)\n",
    "                    .shuffle(buffer_size=buf_size, reshuffle_each_iteration=True)\n",
    "                    .repeat(48).batch(batch_size).prefetch(AUTO))\n",
    "    validation_ds = (validation_ds_dust_aug.concatenate(validation_ds_nodust_aug)\n",
    "                    .repeat(48).batch(batch_size).prefetch(AUTO))\n",
    "\n",
    "else:\n",
    "    # if dust is not activated compute the dataset from the nodust dataset and reapeat the data and batch it\n",
    "    train_ds = (train_ds_nodust_aug\n",
    "                    .shuffle(buffer_size=buf_size, reshuffle_each_iteration=True)\n",
    "                    .repeat(48).batch(batch_size).prefetch(AUTO))\n",
    "    validation_ds = (validation_ds_nodust_aug\n",
    "                    .repeat(48).batch(batch_size).prefetch(AUTO))\n",
    "\n",
    "# check all the datasets to see if they are correct\n",
    "print(validation_ds_nodust_aug)\n",
    "print(train_ds_nodust_aug)\n",
    "print(validation_ds)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualice the data \n",
    "def viz(img, label, pred=np.array([0,0,0]), id=None):\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(img, cmap='gray', vmin=0, vmax=255,origin='lower')\n",
    "    ax.arrow(250,250,100*label[0],100*label[1], head_width=7, head_length=10, fc='g', ec='g')\n",
    "    if np.sum(np.abs(pred)) > 0: ax.arrow(250,250,100*pred[0],100*pred[1], head_width=7, head_length=10, fc='r', ec='r')\n",
    "        \n",
    "    display(Markdown(\"#### **cartesian label:** {}\".format(label)))\n",
    "    if np.sum(np.abs(pred)) > 0: display(Markdown(\"#### **cartesian prediction (x_y):** {}\".format(pred)))\n",
    "    if id: display(Markdown(\"#### **ID:** {}\".format(id)))\n",
    "    plt.show()\n",
    "\n",
    "# visualice the data\n",
    "for idx in range(1,5):\n",
    "    elements = train_ds.take(idx)\n",
    "    for elem in elements:\n",
    "        viz(elem[0][idx], elem[1][idx])#, elem['y_revs'][idx], id=elem['ids'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir {log_dir} --reload_multifile True --host 0.0.0.0 --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON for latter infernce\n",
    "model_json = model.to_json()\n",
    "with open(json_name, \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tensorboard to save the models\n",
    "log_dir_s = log_dir + model_name\n",
    "os.makedirs(log_dir_s, exist_ok=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_s, histogram_freq=1, update_freq='batch', profile_batch=0)\n",
    "\n",
    "#define the learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 200:\n",
    "        return lr #/ 100\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "# Checkpointing\n",
    "checkpointer_1 = tf.keras.callbacks.ModelCheckpoint(filepath=snapshot_weights,\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "checkpointer_2 = tf.keras.callbacks.ModelCheckpoint(filepath=last_snapshot_weights,\n",
    "                               monitor='val_loss',\n",
    "                               verbose=1,\n",
    "                               save_best_only=False)\n",
    "LearningRateScheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "callbacks = [\n",
    "    tensorboard_callback,\n",
    "    checkpointer_1,\n",
    "    checkpointer_2,\n",
    "    LearningRateScheduler,\n",
    "]\n",
    "\n",
    "# Fit the model with the training and validation data, number of epochs and callbacks\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data = validation_ds,\n",
    "    epochs = nb_epochs,\n",
    "    callbacks = callbacks,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the last weights of the model\n",
    "model.save(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking samples from the validation dataset and evaluating the model\n",
    "val_ex, = validation_ds.take(1)\n",
    "train_ex, = train_ds.take(1)\n",
    "\n",
    "images_v, labels_v = val_ex[0], val_ex[1]\n",
    "images_t, labels_t = train_ex[0], train_ex[1]\n",
    "\n",
    "#making the predictions for the taken images\n",
    "outputs_v = model.predict(images_v)\n",
    "outputs_t = model.predict(images_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualicing the results\n",
    "for img,label,pred,i in zip(images_t, labels_t, outputs_t,range(5)):\n",
    "    viz(img, label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img,label,pred,i in zip(images_v, labels_v, outputs_v,range(5)):\n",
    "    viz(img, label, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
